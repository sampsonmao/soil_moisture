---
title: "SMAP Soil Analysis"
author: "Sampson Mao"
date: ''
output:
  pdf_document: default
  html_notebook: default
---

# Exploratory Data Analysis

```{r load_data, message=F}
library(rgdal); library(sp); library(maps); library(maptools); library(dplyr)

# The following script converts the satellite coordinates to longitude/latitude
source("lonlat_smap_data_script.R")
# mydata contains the data
names(mydata) = c("longitude", "latitude", "aug6",
                  "aug7", "aug8", "aug9",
                  "aug10", "aug11", "aug12")

# The following code to convert coordinates to state names is from 
# https://stackoverflow.com/questions/8751497/latitude-longitude-coordinates-to-state-code-in-r
lonlat_to_state_sp <- function(pointsDF) {
    # Prepare SpatialPolygons object with one SpatialPolygon
    # per state (plus DC, minus HI & AK)
    states <- map('state', fill=TRUE, col="transparent", plot=FALSE)
    IDs <- sapply(strsplit(states$names, ":"), function(x) x[1])
    states_sp <- map2SpatialPolygons(states, IDs=IDs,
                     proj4string=CRS("+proj=longlat +datum=WGS84"))
    # Convert pointsDF to a SpatialPoints object 
    pointsSP <- SpatialPoints(pointsDF, 
                    proj4string=CRS("+proj=longlat +datum=WGS84"))
    # Use 'over' to get _indices_ of the Polygons object containing each point 
        indices <- over(pointsSP, states_sp)
    # Return the state names of the Polygons object containing each point
    stateNames <- sapply(states_sp@polygons, function(x) x@ID)
    stateNames[indices]
}

mydata$state = lonlat_to_state_sp(mydata[,1:2])
dak.min = mydata %>%
  dplyr::select(longitude, latitude, state, aug6, aug7, aug8) %>%
  filter(state %in% c("north dakota", "south dakota", "minnesota"))

head(dak.min)
```

```{r usmap_plot,fig.align='center', fig.height=10, fig.width=5.5, message=F, echo=F, warning=F}
library(usmap)
library(ggplot2)
library(reshape2)

transformed_data = usmap_transform(dak.min)
melted = melt(
  transformed_data,
  id.vars = c("longitude", "latitude", "state", "longitude.1", "latitude.1"),
  variable.name = "date"
)

plot_usmap(include = c("ND", "SD", "MN")) +
  geom_point(
    data = melted
    %>% dplyr::select(-date),
    aes(x = longitude.1, y = latitude.1, size = value),
    alpha = 0.1,
    color = "grey"
  ) +
  geom_point(
    data = melted,
    aes(
      x = longitude.1,
      y = latitude.1,
      color = value,
      size = value
    ),
    shape = 16,
    alpha = 0.6
  ) +
  facet_grid(rows= vars(date)) +
  labs(size = "Soil moisture (%)", color= "Soil moisture (%)") +
  scale_color_gradient(guide = guide_legend(),low = 'greenyellow', high = 'forestgreen') +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 16),
    legend.text = element_text(size = 12),
    strip.background = element_blank(),
    strip.text.y.right = element_text(angle = 0, size=12)
  )
ggsave("birdseye.pdf", width=5.5, height=10, dpi=600)
```

```{r drop_plot ,fig.align='center', echo=F}
library(scatterplot3d)
logit = function(x) log(x/(1-x))

soil = melted[complete.cases(melted),] 
x = soil$longitude.1
y = soil$latitude.1
z = logit(soil$value)
borders = us_map(include = c("ND", "SD", "MN"))

# https://stackoverflow.com/questions/9946630/colour-points-in-a-plot-differently-depending-on-a-vector-of-values
rbPal <- colorRampPalette(c('black','red'))
z.col = rbPal(10)[as.numeric(cut(z,breaks = 10))]

par(mar=c(1,2,0,1))

#pdf("balloonslogit.pdf", width=7, height=7)

s = scatterplot3d(borders$x, borders$y, rep(0,length(borders$x))-3, 
                  type="l", box=F, axis=T, grid=F,
                  zlim=c(-3,3), color=4, angle=65,
                  x.ticklabs="", y.ticklabs="",
                  xlab="", ylab="", zlab="logit(Moisture)")
s$points3d(x,y,z, type="h", pch=16, col=z.col)
title("Soil Moisture in the Dakota-Minnesota region", line=-1)

#dev.off()
```

There looks to be evidence of a second order trend as a function of location. In the balloons plot, we can see that the northeastern part of minnesota has a much higher soil moisture content compared to the rest of the state and the Dakotas. For the rest of the region, there is also a high or low moisture content depending on location, though it is not as large as northeast Minnesota. A constant trend would not be able to explain the sudden increase as we move into the more northeastern regions. So for our covariates matrix $D$, we have

$$
D = \begin{bmatrix} \mathbf{1} & \mathbf{d_1} & \mathbf{d_2} & \mathbf{d_1}^2 & \mathbf{d_2}^2 & \mathbf{d_1 d_2}
\end{bmatrix}
$$

where $\mathbf{d_1},\mathbf{d_2}$ are the vector of longitudes and latitudes respectively. For the rest of the homework, the jittered coordinates are used, except for the variogram plots so that we can estimate the semivariance for 0 distance. Some matrix calculations may not be possible with un-jittered data because of having 0 distance. The jitter will be very small, with the maximum jitter value being $10^{-7}$.

```{r boxcox, fig.align='center', echo=F, message=F, warning=F}
library(geoR)
# Raw data
soil_raw = soil[,c(1,2,7)]

# Jittering replicates
soil_jitter = data.frame(jitterDupCoords(soil[,1:2], max=1e-7), value=soil$value)

# Lists for geoR
in_geor = list(coords=soil_raw[,1:2], data=logit(soil_raw$value))
in_geor_jitter = list(coords=soil_jitter[,1:2], data=logit(soil_jitter$value))

# Creating design matrix D
Y = soil_jitter$value
n = length(Y)
lon = soil_jitter$longitude
lat = soil_jitter$latitude
D = cbind(rep(1,n), lon, lat, lon^2, lat^2, lon*lat)
colnames(D) = c("Intercept", "lon", "lat", "lon^2", "lat^2", "lon:lat")

# Boxcox comparison
layout.matrix = matrix(c(1,2,3,4),2,2)
layout(layout.matrix)
par(oma = c(5, 4, 2, 0) + 0.1,
    mar = c(2, 4, 2, 1) + 0.1)
hist(Y, main="No Transformation",
     xlab="Soil moisture (%)",
     breaks=20)
hist(log(Y), main="Log transform",
     xlab="log(Soil Moisture)",
     breaks=20)
hist(logit(Y), main="Logit transform",
     xlab="logit(Soil Moisture)",
     breaks=20)
MASS::boxcox(lm(Y ~ D))
```

When plotting the histogram for the data, we can see that it is skewed to the right, which indicates non-normality (upper left hand plot). The bottom right hand plot is the Box-Cox fit of the data, using $D$ is the covariates matrix, for various values of $\lambda$. We can see that very small values of $\lambda$, including 0, are in the 95\% confidence interval for the parameter. However the data only has values between 0 and 1, and the log transform will only map them onto $(-\infty,0)$. Logit and tanh transforms are also considered, and it looks like the logit transform is comparable to the log transform, so we will use the logit transform for the rest of the analysis.

Next is to look at the empirical variograms and the directional variogram to check for anisotropy. 

```{r empirical_variograms,fig.align='center', message=F, fig.height=5, fig.width=9, echo=F}
par(pty="s", mfrow=c(2,2))

vgm.notrend = variog(in_geor, lambda=1, trend="2nd", messages=F)
vgm4.notrend = variog4(in_geor_jitter, lambda=1, trend="2nd", messages=F)

pdf("figures/variogram.pdf", width=5, height=5)
par(mfrow = c(1,1))
# plot(vgm.notrend, col="blue", pch=4, main="Empirical variogram")
# lines(vgm.notrend, col="blue", pch=4)
plot(vgm4.notrend, omni=T)
title("Directional variogram ")
title(xlab="Distance", ylab="Semivariance", outer=T)
dev.off()
```

In the above plots created by geoR, we can see that the residuals (trend removed data) are normally distributed, which is a good sign for our model. Furthermore, there appears to be a near-costant variance in both the X and Y (longitude and latitude) directions, shown in the bottom left and top right plots. Across the states, there seem to be more variability than up and down the states based on these 2 plots. Finally, in the top left plot, we can see that the values are more consistent. The red x denotes the fourth quartile, which we see exists in the Dakotas now rather than just northeast Minnesota. Similarly for the yellow +, green triangles, and blue circles for the third, second, and first quartiles respectively.

# Classical analysis

In classical parameter estimation we assume that the mean function is a linear combination of spatial explanatory variabiles. For our data, this would apply to the logit of the mean function as we saw before. Here, we don't know about any existing covariance structure, so we can use ordinary least squares to estimate the trend.

$$
\begin{aligned}
logit(\mu(s)) &= \beta_0 + \beta_1\mathbf{d_1} + \beta_2\mathbf{d_2} + \beta_3\mathbf{d_1^2} + \beta_4\mathbf{d_2^2} + \beta_5\mathbf{d_1 d_2} \\
D^TD \hat{\beta} &= D^T \log(Y) \\
R &= \log(Y) - D \hat{\beta}
\end{aligned}
$$

```{r}
library(stargazer)
DD = D[,2:6]
ols = lm(logit(Y) ~ DD)
output = summary(ols)
stargazer(ols, single.row = T, intercept.top = T, intercept.bottom=F, type="text")
```

We have the residuals (binned) $r^*_k$ and we want to fit a curve $V(x^*_k;\sigma^2, \phi, \tau^2)$, with corresponding distance $x^*_k$. A $n_k$-weighted objective function is

$$
S(\sigma^2, \phi, \tau^2) = \sum_{k=1}^n n_k \left\{r^*_k - V(x^*_k;\sigma^2, \phi, \tau^2)\right\}^2
$$

where $n_k$ is the number of points used to obtain the binned residual $v_k$. Minimizing $S$ with respect to the parameters would give us the estimated curve for our data. In this case, we would like $V$ to be the Matern covariance function with nugget $\tau^2$. This is implemented in the `variofit` function in `geoR`

```{r variogram_LSE,fig.align='center', fig.height=9, fig.width=9, echo=F}
vgm = variog(in_geor, lambda=1, trend="2nd", nugget.tolerance = 0, messages=F)

init.pars = c(0.15, 2)

fit0.5 = variofit(vgm, kappa=0.5, cov.model="matern", ini.cov.pars = init.pars, 
                  limits=c(0,10), messages=F)
fit1 = variofit(vgm, kappa=1, cov.model="matern", ini.cov.pars = init.pars, 
                limits=c(0,10), messages=F)
fit1.5 = variofit(vgm, kappa=1.5, cov.model="matern", ini.cov.pars = init.pars, 
                  limits=c(0,10), messages=F)
fit2.5 = variofit(vgm, kappa=2.5, cov.model="matern", ini.cov.pars = init.pars, 
                  limits=c(0,10), messages=F)

pdf("figures/OLS.pdf", width=8, height=8)
par(mfrow = c(2, 2), pty="s",
    oma = c(5, 4, 2, 0) + 0.1,
    mar = c(1, 1, 1, 1) + 0.1)
plot(vgm.notrend, pch=16, main=bquote(nu==0.5), xlab="", ylab="", xaxt='n')
lines(fit0.5, col="blue", lty=4, lwd=4)
abline(v=fit0.5$practicalRange, lty=3)
abline(h=fit0.5$cov.pars[1]+fit0.5$nugget, lty=2)
arrows(0,0,0, fit0.5$nugget, angle=20, length=0.1, lwd=2, col="cyan")
arrows(0,0,fit0.5$cov.pars[2], 0, angle=20, length=0.1, lwd=2, col="brown")
text(7,0, labels=bquote("SSE="~.(round(fit0.5$value, 2))))

plot(vgm.notrend, pch=16, main=bquote(nu==1.0), xlab="", ylab="", xaxt='n', yaxt='n')
lines(fit1, col="orange", lty=4, lwd=4)
abline(v=fit1$practicalRange, lty=3)
abline(h=fit1$cov.pars[1]+fit1$nugget, lty=2)
arrows(0,0,0, fit1$nugget, angle=20, length=0.1, lwd=2, col="cyan")
arrows(0,0,fit1$cov.pars[2], 0, angle=20, length=0.1, lwd=2, col="brown")
text(7,0, labels=bquote("SSE="~.(round(fit1$value, 2))))

plot(vgm.notrend, pch=16, main=bquote(nu==1.5), xlab="", ylab="")
lines(fit1.5, col="red", lty=4, lwd=4)
abline(v=fit1.5$practicalRange, lty=3)
abline(h=fit1.5$cov.pars[1]+fit1.5$nugget, lty=2)
arrows(0,0,0,fit1.5$nugget, angle=20, length=0.1, lwd=2, col="cyan")
arrows(0,0,fit1.5$cov.pars[2], 0, angle=20, length=0.1, lwd=2, col="brown")
text(7,0, labels=bquote("SSE="~.(round(fit1.5$value, 2))))

plot(vgm.notrend, pch=16, main=bquote(nu==2.5), xlab="", ylab="", yaxt='n')
lines(fit2.5, col="green", lty=4, lwd=4)
abline(v=fit2.5$practicalRange, lty=3)
abline(h=fit2.5$cov.pars[1]+fit2.5$nugget, lty=2)
arrows(0,0,0,fit2.5$nugget, angle=20, length=0.1, lwd=2, col="cyan")
arrows(0,0,fit2.5$cov.pars[2], 0, angle=20, length=0.1, lwd=2, col="brown")
text(7,0, labels=bquote("SSE="~.(round(fit2.5$value, 2))))
title(main="Least square fits with increasing smoothness", 
      xlab="Distance",
      ylab="Semivariance",
      outer=T, line=1)
legend("bottomright", legend=c("Sill", "Range", "Nugget", "Pract. Rnge", "Fit", "Empirical"),
       col=c("black", "brown", "cyan", "black", "black", "black"), lty=c(2,1,1,3,4,NA),
       pch=c(NA,NA,NA,NA,NA,16), lwd=c(1,2,2,1,1), bty="n")
dev.off()
```

Since we removed the trend, the likelihood is

$$
\begin{aligned}
L(\psi) &\propto |V(\psi)|^{-1/2} \exp \left\{-\frac{1}{2}R^TV(\psi)^{-1}R\right\} \\
\log(L(\psi)) &\propto - \frac{1}{2}\log|V(\psi)| -\frac{1}{2}R^TV(\psi)^{-1}R
\end{aligned}
$$

We have the entries of $V$ given by $V(\psi)_{ij} = C(s_i,s_j;\sigma^2, \phi) + \tau^2\delta_{ij}$, where $C$ is the Matern covariance function. This is implemented in the R code below

```{r}
logL = function(sigma2, phi, nu, tau2) {
  V.mat = varcov.spatial(in_geor_jitter$coords, cov.pars=c(sigma2, phi), nugget=tau2, kappa=nu)
  V = V.mat$varcov
  
  n = dim(D)[1]
  
  L = t(chol(V))
  Z = forwardsolve(L, logit(Y))
  FF = forwardsolve(L, D)
  DVD = t(FF) %*% FF
  
  # R_1 beta.hat = (Q^T Y)_1
  # S^2 = ||(Q^TY)_2||
  QR = qr(FF)
  Q = qr.Q(QR, complete=T)
  R = qr.R(QR, complete=T)
  
  QZ = t(Q) %*% Z
  
  R1 = R[1:k,]
  QZ1 = QZ[1:k,]
  QZ2 = QZ[(k+1):n,]
  
  beta.hat = backsolve(R1, QZ1)
  S2 = t(QZ2) %*% QZ2

  log.det = 2*sum(log(diag(L)))
  
  density = -n/2*log(2*pi) -0.5*log.det - 0.5*S2
  
  return(density)
}

vect.logL = Vectorize(logL, vectorize.args=c("sigma2", "phi"))
```

```{r 2D_density, echo=F}
sill0.5 = seq(0.01, 1, l=20)
range0.5 = seq(0.1, 15, l=20)

sill1 = seq(0.01, 1, l=20)
range1 = seq(0.1, 3, l=20)

sill1.5 = seq(0.01, 1, l=20)
range1.5 = seq(0.1, 2, l=20)

logdensity0.5 = outer(sill0.5, range0.5, function(x,y) vect.logL(x,y, nu=0.5, tau2=fit0.5$nugget))
logdensity1 = outer(sill1, range1, function(x,y) vect.logL(x,y, nu=1, tau2=fit1$nugget))
logdensity1.5 = outer(sill1.5, range1.5, function(x,y) vect.logL(x,y, nu=1.5, tau2=fit1.5$nugget))
logdensity2.5 = outer(sill1.5, range1.5, function(x,y) vect.logL(x,y, nu=2.5, tau2=fit2.5$nugget))
```

```{r contours,fig.align='center', fig.height=9, fig.width=9, echo=F}
par(pty="s", mfrow=c(2,2),
    oma = c(5, 4, 2, 0) + 0.1,
    mar = c(2, 3, 2, 1) + 0.1)
contour(sill0.5, range0.5, logdensity0.5-max(logdensity0.5),
        xlab=expression(sigma^2),
        ylab=expression(phi),
        main=bquote(nu=="0.5"))
contour(sill1, range1, logdensity1-max(logdensity1),
        xlab=expression(sigma^2),
        ylab=expression(phi),
        main=bquote(nu=="1"))
contour(sill1.5, range1.5, logdensity1.5-max(logdensity1.5),
        xlab=expression(sigma^2),
        ylab=expression(phi),
        main=bquote(nu=="1.5"))
contour(sill1.5, range1.5, logdensity2.5-max(logdensity2.5),
        xlab=expression(sigma^2),
        ylab=expression(phi),
        main=bquote(nu=="2.5"))
title("Log likelihood function with smoothness nu", outer=T)
```

# Bayesian analysis

$$
\begin{aligned}
L(\beta, \sigma^2, \tau^2, \phi|\boldsymbol{x}) 
&\propto \det(\sigma^2 \rho(\phi) + \tau^2I)^{-1/2} \exp\left\{-\frac{1}{2}(X-D\beta)^T(\sigma^2 \rho(\phi) + \tau^2I)^{-1}(X-D\beta)\right\} \\
L(\beta, \sigma^2, \theta, \phi|\boldsymbol{x}) 
&\propto \det(\theta\rho(\phi) + (1-\theta)I)^{-1/2} (\sigma^2)^{-m/2} \exp\left\{-\frac{1}{2\sigma^2}(X-D\beta)^T(\theta\rho(\phi) + (1-\theta)I)^{-1}(X-D\beta)\right\} \\
&\propto \det(V(\theta, \phi))^{-1/2} \exp\left\{-\frac{1}{2\sigma^2}\left[(\beta-\hat{\beta})^TD^T V(\theta, \phi)^{-1} D (\beta-\hat{\beta}) + S^2(\phi)\right]\right\} \\
\end{aligned}
$$

where

$$
\begin{aligned}
D^TV(\theta, \phi)^{-1}D\hat{\beta} &= D^TV(\theta, \phi)^{-1}X\\
S^2(\theta, \phi) &= (X-D\hat{\beta})^T V(\theta, \phi)^{-1}(X-D\hat{\beta})\\
\end{aligned}
$$

$$
C(\sigma^2,\phi,\tau^2) = \sigma^2\rho(\phi) + \tau^2I \\
\theta = \frac{\sigma^2}{\tau^2+\sigma^2}\\
C(\sigma^2,\phi,\theta) = \sigma^2(\theta\rho(\phi) + (1-\theta)I) \\
C(\sigma^2,\phi,\theta) = \sigma^2V(\theta, \phi) 
$$


$$
\begin{aligned}
p(\beta, \sigma^2, \theta, \phi) &= p(\beta, \sigma^2|  \theta, \phi) p(\theta, \phi) \\
p(\beta, \sigma^2, \theta, \phi) &= IG(\sigma^2 \mid a,b) p(\theta, \phi) \\
\pi(\beta, \sigma^2, \theta, \phi| \boldsymbol{x}) &\propto f(\boldsymbol{x} | \beta, \sigma^2, \theta, \phi) p(\beta, \sigma^2, \theta, \phi) \\
&\propto |V(\theta, \phi)|^{-1/2} \exp\left\{-\frac{1}{2\sigma^2}\left[(\beta-\hat{\beta})^TD^T V(\theta, \phi)^{-1} D (\beta-\hat{\beta}) + S^2(\theta, \phi)\right]\right\} \\
&\phantom{\propto} (\sigma^2)^{-\frac{m-k}{2}-a-1} \exp\left\{-\frac{b}{\sigma^2}\right\}
 p(\theta, \phi)\\
\pi(\sigma^2, \theta, \phi| \boldsymbol{x}) 
&\propto |V(\theta, \phi)|^{-1/2} |D^TV(\theta, \phi)^{-1}D|^{-1/2} \exp\left\{- \frac{1}{2\sigma^2} S^2(\theta, \phi)\right\}\\
&\phantom{\propto} (\sigma^2)^{-\frac{m-k}{2}-a-1} \exp\left\{-\frac{b}{\sigma^2}\right\}  p(\theta, \phi) \\
\pi(\theta, \phi| \boldsymbol{x}) 
&\propto |V(\theta, \phi)|^{-1/2} |D^TV(\theta, \phi)^{-1}D|^{-1/2} (S(\theta, \phi)+2b)^{-\frac{m-k}{2}-a} p(\theta, \phi)
\end{aligned}
$$
$$
\begin{aligned}
\pi(\beta | \sigma^2, \tau^2, \phi, \boldsymbol{x}) 
&\propto \exp\left\{-\frac{1}{2\sigma^2}(\beta-\hat{\beta})^TD^T V(\theta, \phi)^{-1} D (\beta-\hat{\beta})\right\} \\
\pi(\sigma^2 | \theta, \phi, \boldsymbol{x}) 
&= (\sigma^2)^{-\frac{m-k}{2}-a-1} \exp\left\{-\frac{S^2(\theta, \phi) +2b}{2\sigma^2}\right\} \\
\pi(\theta, \phi| \boldsymbol{x}) 
&\propto |V(\theta, \phi)|^{-1/2} |D^TV(\theta, \phi)^{-1}D|^{-1/2}  (S^2(\theta, \phi)+2b)^{-\frac{m-k}{2}-a} p(\theta,\phi)
\end{aligned}
$$

```{r }
library(mvtnorm)

sigmoid = function(x) 1/(1+exp(-x))

n = dim(D)[1]
k = dim(D)[2]

# Hyperparameters for sigma^2
a = 1
b = 0.0001

# Hyperparameters for theta
alph = 2
bet = 0.5

# Hyperparameters for phi
ss = 2
u = 0

thetaphi.post = function(t, p) {
  theta = sigmoid(t)
  phi = exp(p)
  # theta = sigma^2/(tau^2 + sigma^2)
  I = diag(n)
  rho.mat = varcov.spatial(in_geor_jitter$coords, cov.pars=c(1, phi), nugget=0, kappa=0.5)
  rho = rho.mat$varcov
  V = theta*rho + (1-theta)*I

  # Z = L^-1 Y
  # F = L^-1 D
  L = t(chol(V))
  # L.inv = forwardsolve(L, I)
  Z = forwardsolve(L, logit(Y))
  FF = forwardsolve(L, D)
  DVD = t(FF) %*% FF
  
  # R_1 beta.hat = (Q^T Y)_1
  # S^2 = ||(Q^TY)_2||
  QR = qr(FF)
  Q = qr.Q(QR, complete=T)
  R = qr.R(QR, complete=T)
  
  QZ = t(Q) %*% Z
  
  R1 = R[1:k,]
  QZ1 = QZ[1:k,]
  QZ2 = QZ[(k+1):n,]
  
  beta.hat = backsolve(R1, QZ1)
  S2 = t(QZ2) %*% QZ2

  # Find inverse of (D V^-1 D)^-1 for beta covariance
  # F^-1 = R^-1 Q^T
  F.inv = backsolve(R, t(Q))
  DVD.inv = F.inv %*% t(F.inv)
  
  # Determinants
  E = t(chol(DVD))
  logdet.V = 2*sum(log(diag(L)))
  logdet.DVD = 2*sum(log(diag(E)))
  
  # Priors
  # Beta for theta
  theta.prior = (alph-1)*log(theta) + (bet-1)*log(1-theta)
  # Lognormal for phi
  phi.prior = -log(phi) -0.5*log(ss) - (log(phi)-u)^2/(2*ss)

  logdensity = - 0.5*logdet.V - 0.5*logdet.DVD - ((n-k)/2 + a)*log(S2 + 2*b) + theta.prior + phi.prior + (-t + 2*log(exp(-t)+1)) + p
  
  out = list(logdensity=logdensity, beta.hat=beta.hat, S2=S2, DVD.inv=DVD.inv)
  return(out)
}
```

```{r}
set.seed(100)
n.iter = 10999
beta.mcmc = matrix(c( -102, -0.392, 3.58, 0.00141, -0.0212, 0.0158), ncol=6, nrow=n.iter+1, byrow=T)
sigma.mcmc = rep(0.3, n.iter+1)

#Starting points below are in the transformed scale
phi.mcmc = rep(0, n.iter+1)
theta.mcmc = rep(2, n.iter+1)
sigma.tune = .5
sigma.matrix = matrix(c(0.1316893,0.1,0.1,0.2091652), ncol=2, nrow=2)
accepted = 0
for (i in 1:n.iter) {
  prop = rmvnorm(1, c(theta.mcmc[i],phi.mcmc[i]), sigma=sigma.tune*sigma.matrix)
  theta.prop = prop[1]
  phi.prop = prop[2]
  
  pi.p = thetaphi.post(theta.prop, phi.prop)
  pi.c = thetaphi.post(theta.mcmc[i], phi.mcmc[i])
  
  log.ratio = pi.p$logdensity - pi.c$logdensity
    
  if (log(runif(1)) < log.ratio) {
    accepted = accepted + 1
    phi.mcmc[i+1] = phi.prop
    theta.mcmc[i+1] = theta.prop
    sigma.mcmc[i+1] = 1/rgamma(1, (n+k)/2+a, (pi.p$S2 + 2*b)/2)
    beta.mcmc[i+1,] = rmvnorm(1, pi.p$beta.hat, sigma.mcmc[i+1]*pi.p$DVD.inv)
    } else {
    phi.mcmc[i+1] = phi.mcmc[i]
    theta.mcmc[i+1] = theta.mcmc[i]
    sigma.mcmc[i+1] = sigma.mcmc[i]
    beta.mcmc[i+1,] = beta.mcmc[i,]
  }
  if (i %% 20 == 0) print(i)
}

par(mfrow=c(2,2))
plot(exp(phi.mcmc), type="l")
plot(1/(1+exp(-theta.mcmc)), type="l")
plot(sigma.mcmc, type="l")
plot(beta.mcmc[,1], type="l")
accepted/n.iter
```


```{r}
samples = cbind(beta.mcmc[1001:11000,],
                sigma.mcmc[1001:11000],
                1/(1+exp(-theta.mcmc))[1001:11000],
                exp(phi.mcmc)[1001:11000])
colnames(samples) = c("beta0", "beta1", "beta2", "beta3", "beta4", "beta5",
                      "sigma", "theta", "phi")
# write.csv(samples, "mcmc_logit_lognormal-beta-05.csv", row.names=F)
# mcmc = read.csv("mcmc_logit_lognormal-beta-05.csv")
```


```{r}
optim.MAP = optim(c(0.2,1), function(x) thetaphi.post(x[1],x[2])$logdensity, control=list(fnscale=-1))
transformed.MAP = optim.MAP$par
theta.MAP = sigmoid(transformed.MAP[1])
phi.MAP = exp(transformed.MAP[2])

mcmc0.5 = read.csv("mcmc_logit_lognormal-beta-05-2.csv")
mcmc1.0 = read.csv("mcmc_logit_lognormal-beta-10.csv")
mcmc1.5 = read.csv("mcmc_logit_lognormal-beta-15.csv")
mcmc2.5 = read.csv("mcmc_logit_lognormal-beta-25.csv")

theta.mode0.5 = names(sort(-table(mcmc0.5$theta)))[1]
phi.mode0.5 = names(sort(-table(mcmc0.5$phi)))[1]
theta.mode1.0 = names(sort(-table(mcmc1.0$theta)))[1]
phi.mode1.0 = names(sort(-table(mcmc1.0$phi)))[1]
theta.mode1.5 = names(sort(-table(mcmc1.5$theta)))[1]
phi.mode1.5 = names(sort(-table(mcmc1.5$phi)))[1]
theta.mode2.5 = names(sort(-table(mcmc2.5$theta)))[1]
phi.mode2.5 = names(sort(-table(mcmc2.5$phi)))[1]
```

```{r}
library(stringr)
mcmc.samples = data.frame(mcmc0.5, mcmc1.0, mcmc1.5, mcmc2.5)
stacked.mcmc = stack(mcmc.samples)
stacked.mcmc$variable = str_remove(stacked.mcmc$ind, "\\..*")
CI = ggplot(data=stacked.mcmc) + 
  stat_summary(mapping=aes(x=ind, y=values),
    fun.min = function(z) quantile(z,0.025),
    fun.max = function(z) quantile(z,0.975),
    fun = mean) + 
  labs(x="Smoothness", y="Value",
       title="95% Credible Intervals")+
  facet_wrap(vars(variable), scales="free",) +
  theme_minimal() +
  theme(axis.text.x = element_text()) +
  scale_x_discrete(labels= c(0.5,1.0,1.5,2.5))
CI

ggsave("figures/credible_intervals.pdf", CI, dpi=600, height=5, width=5)
```
```{r}
stacked2 = stacked.mcmc[stacked.mcmc$variable %in% c("sigma", "phi"),]
stacked2 = stacked2[!stacked2$ind %in% c("sigma", "phi"),]
CI2 = ggplot(data=stacked2) + 
  stat_summary(mapping=aes(x=ind, y=values),
    fun.min = function(z) quantile(z,0.025),
    fun.max = function(z) quantile(z,0.975),
    fun = mean) + 
  labs(x="Smoothness", y="Value")+
  facet_wrap(vars(variable), scales="free", ncol=1) +
  theme_minimal() +
  theme(axis.text.x = element_text()) +
  scale_x_discrete(labels= c(0.5,1.0,1.5,2.5))
CI2
ggsave("figures/sigmaphiCI.pdf", CI2, height=5, width=3, dpi=600)
```

## Posterior predictive inference

The posterior predictive distribution following integral, where all of the parameters are averaged over. Let $W_1$ be the observed data and $W_0$ be the predicted data

$$
\begin{aligned}
p(W_0|w_1) &= \iiiint p(W_0 \mid w_1 \,;\, \beta, \sigma^2, \theta, \phi) \, \pi(\beta, \sigma^2, \theta, \phi \mid w_1) \, d\beta \, d\sigma^2 \, d\theta \, d\phi \\
&\approx \frac{1}{G}\sum_{g=1}^G p(W_0 \mid w_1 \,;\, \beta^{(g)}, [\sigma^{2}]^{(g)}, \theta^{(g)}, \phi^{(g)})
\end{aligned}
$$
The conditional distribution of $W_0|W_1$ is given by the formulas for the conditional MVN distribution. the $\,;\,$ emphasizes that the variables that come after are not being conditioned on and are parameters.

$$
\begin{aligned}
(W_0, W_1) &\sim N(\mu, \Sigma), \\
\Sigma &= 
\begin{pmatrix}
\Sigma_{W_0,W_0} & \Sigma_{W_0,W_1} \\
\Sigma_{W_1,W_0} & \Sigma_{W_1,W_1}
\end{pmatrix}
\\
W_0|W_1 &\sim N(\mu_{W_0|W_1}, \Sigma_{W_0|W_1}) \\
\mu_{W_0|W_1} &= \mu_{W_0} + \Sigma_{W_0,W_1} \Sigma_{W_1,W_1}^{-1}(W_{1} - \mu_{W_1}) \\
\mu_{W_0} &= D_{W_0}\beta\\
\mu_{W_1} &= D_{W_1}\beta\\
\Sigma_{W_0|W_1} &= \Sigma_{W_0,W_0} - \Sigma_{W_0,W_1}\Sigma_{W_1,W_1}^{-1}\Sigma_{W_1|W_0} \\
\Sigma_{W_1,W_1} &= C(\sigma^2, \theta, \phi) = \sigma^2(\theta\rho(\phi) + (1-\theta)I) = \omega^2\rho(\phi) + \tau^2I \\
\Sigma_{W_0,W_1} &=\sigma^2\theta\rho(\phi) = \omega^2\rho(\phi) \\
\Sigma_{W_0,W_0} &=\sigma^2\theta\rho(\phi) = \omega^2\rho(\phi) \\
\end{aligned}
$$

```{r}
library(plgp)
r = distance(in_geor_jitter$coords)
exponential = function(x, phi) exp(-x/phi)

reps = dim(mcmc)[1]
W0.1 = matrix(0, nrow=n, ncol=reps)
for (i in 1:reps) {
  # Sigma_W1,W1
  #rho.mat = varcov.spatial(in_geor_jitter$coords, cov.pars=c(1, mcmc$phi[i]), nugget=0, kappa=0.5)
  #rho = rho.mat$varcov
  rho = exponential(r, mcmc$phi[i])
  I = diag(n)
  V = mcmc$theta[i]*rho + (1-mcmc$theta[i])*I
  S11 = C = mcmc$sigma[i]*V
  
  # (Sigma_W1,W1)^-1
  LS11 = t(chol(S11))
  LS11.inv = forwardsolve(LS11, I)
  S11.inv = t(LS11.inv) %*% LS11.inv
  
  # Sigma_W0,W0; W0,W1; and W1,W0
  S00 = S01 = S10 = mcmc$sigma[i] * mcmc$theta[i] * rho
  
  # Sigma_W0|W1
  S0.1 = S00 - S01 %*% S11.inv %*% S10
  
  # mu_W0|W1
  mu0.1 = D %*% t(mcmc[i,1:6]) + S01 %*% (S11.inv %*% (logit(Y) - D %*% t(mcmc[i,1:6])))
  
  W0.1[,i] = rmvnorm(1,mu0.1, S0.1, checkSymmetry = F)
  if (i %% 20 == 0) print(i)
}
write.csv(W0.1, "conditional_samples_05.csv", row.names=F)
```

```{r}
PW = read.csv("conditional_samples_05-2.csv")
PY = sigmoid(PW)
post.mean = rowMeans(PY)
post.var = apply(PY,1,var)
post.stats = data.frame(long=soil_raw$longitude, lat=soil_raw$latitude,
                        pmean=post.mean, pvar=post.var)

PW1 = read.csv("conditional_samples_10.csv")
PY1 = sigmoid(PW1)
post.mean1 = rowMeans(PY1)
post.var1 = apply(PY1,1,var)
post.stats1 = data.frame(long=soil_raw$longitude, lat=soil_raw$latitude,
                        pmean=post.mean1, pvar=post.var1)

PW1.5 = read.csv("conditional_samples_15.csv")
PY1.5 = sigmoid(PW1.5)
post.mean1.5 = rowMeans(PY1.5)
post.var1.5 = apply(PY1.5,1,var)
post.stats1.5 = data.frame(long=soil_raw$longitude, lat=soil_raw$latitude,
                        pmean=post.mean1.5, pvar=post.var1.5)

PW2.5 = read.csv("conditional_samples_25.csv")
PY2.5 = sigmoid(PW2.5)
post.mean2.5 = rowMeans(PY2.5)
post.var2.5 = apply(PY2.5,1,var)
post.stats2.5 = data.frame(long=soil_raw$longitude, lat=soil_raw$latitude,
                        pmean=post.mean2.5, pvar=post.var2.5)
```

```{r, fig.width=5, fig.height=2.5}
library(viridis)
MainStates <- map_data("state")
NewStates <- MainStates %>% filter(region %in% c("north dakota", "south dakota", "minnesota"))

rng = range(Y)

orig = ggplot() +
  geom_tile(data=soil_raw, aes(x=longitude, y=latitude, fill=value), height=0.5) +
  scale_fill_viridis(name="Moisture (%)", limits=c(rng[1], rng[2])) +
  geom_path(data=NewStates, aes(x=long, y=lat, group=group), colour = "grey60") +
  labs(title="Soil Moisture Content", x="Longitude", y="Latitude") +
  theme_minimal()

postmean = ggplot() +
  geom_tile(data=post.stats, aes(x=long, y=lat, fill=pmean), height=0.5) +
  scale_fill_viridis( name="Moisture (%)", limits=c(rng[1], rng[2])) +
  geom_path(data=NewStates, aes(x=long, y=lat, group=group), colour = "grey60") +
  labs(title=bquote("Posterior Predictive Mean Moisture"~nu==0.5), x="Longitude", y="Latitude") +
  theme_minimal()

postvar =ggplot() +
  geom_tile(data=post.stats, aes(x=long, y=lat, fill=pvar), height=0.5) +
  scale_fill_viridis( name="Variance") +
  geom_path(data=NewStates, aes(x=long, y=lat, group=group), colour = "grey60") +
  labs(title=bquote("Posterior Predictive Moisture Variance"~nu==0.5), 
       x="Longitude", y="Latitude") +
  theme_minimal()
  
orig
postmean
postvar
ggsave("figures/orig.pdf", orig, dpi=600, width=5, height=3)
ggsave("figures/postmean.pdf", postmean, dpi=600, width=5, height=3)
ggsave("figures/postvar.pdf", postvar, dpi=600, width=5, height=3)
```

```{r, fig.width=5, fig.height=2.5}
library(viridis)
MainStates <- map_data("state")
NewStates <- MainStates %>% filter(region %in% c("north dakota", "south dakota", "minnesota"))

rng = range(Y)


postmean1 = ggplot() +
  geom_tile(data=post.stats1, aes(x=long, y=lat, fill=pmean), height=0.5) +
  scale_fill_viridis( name="Moisture (%)", limits=c(rng[1], rng[2])) +
  geom_path(data=NewStates, aes(x=long, y=lat, group=group), colour = "grey60") +
  labs(title=bquote("Posterior Predictive Mean Moisture"~~nu==1.0), x="Longitude", y="Latitude") +
  theme_minimal()

postvar1 =ggplot() +
  geom_tile(data=post.stats1, aes(x=long, y=lat, fill=pvar), height=0.5) +
  scale_fill_viridis( name="Variance") +
  geom_path(data=NewStates, aes(x=long, y=lat, group=group), colour = "grey60") +
  labs(title=bquote("Posterior Predictive Moisture Variance"~nu==1.0), x="Longitude", y="Latitude") +
  theme_minimal()
  
orig
postmean1
postvar1
ggsave("figures/postmean1.pdf", postmean1, dpi=600, width=5, height=3)
ggsave("figures/postvar1.pdf", postvar1, dpi=600, width=5, height=3)
```

```{r, fig.width=5, fig.height=2.5}
postmean1.5 = ggplot() +
  geom_tile(data=post.stats1.5, aes(x=long, y=lat, fill=pmean), height=0.5) +
  scale_fill_viridis( name="Moisture (%)", limits=c(rng[1], rng[2])) +
  geom_path(data=NewStates, aes(x=long, y=lat, group=group), colour = "grey60") +
  labs(title=bquote("Posterior Predictive Mean Moisture"~nu==1.5), x="Longitude", y="Latitude") +
  theme_minimal()

postvar1.5 =ggplot() +
  geom_tile(data=post.stats1.5, aes(x=long, y=lat, fill=pvar), height=0.5) +
  scale_fill_viridis( name="Variance") +
  geom_path(data=NewStates, aes(x=long, y=lat, group=group), colour = "grey60") +
  labs(title=bquote("Posterior Predictive Moisture Variance"~nu==1.5), x="Longitude", y="Latitude") +
  theme_minimal()
  
orig
postmean1.5
postvar1.5
ggsave("figures/postmean1.5.pdf", postmean1.5, dpi=600, width=5, height=3)
ggsave("figures/postvar1.5.pdf", postvar1.5, dpi=600, width=5, height=3)
```

```{r, fig.width=5, fig.height=2.5}
postmean2.5 = ggplot() +
  geom_tile(data=post.stats2.5, aes(x=long, y=lat, fill=pmean), height=0.5) +
  scale_fill_viridis( name="Moisture (%)", limits=c(rng[1], rng[2])) +
  geom_path(data=NewStates, aes(x=long, y=lat, group=group), colour = "grey60") +
  labs(title=bquote("Posterior Predictive Mean Moisture"~nu==2.5), x="Longitude", y="Latitude") +
  theme_minimal()

postvar2.5 =ggplot() +
  geom_tile(data=post.stats2.5, aes(x=long, y=lat, fill=pvar), height=0.5) +
  scale_fill_viridis( name="Variance") +
  geom_path(data=NewStates, aes(x=long, y=lat, group=group), colour = "grey60") +
  labs(title=bquote("Posterior Predictive Moisture Variance"~nu==2.5), x="Longitude", y="Latitude") +
  theme_minimal()
  
orig
postmean2.5
postvar2.5
ggsave("figures/postmean2.5.pdf", postmean2.5, dpi=600, width=5, height=3)
ggsave("figures/postvar2.5.pdf", postvar2.5, dpi=600, width=5, height=3)
```

```{r}
library(reshape2)
is.highmoisture = (soil_raw$longitude<=-96 & soil_raw$longitude>=-100) & (soil_raw$latitude>=45 & soil_raw$latitude<=47)
highmoist = Y[is.highmoisture]
highmoist.reps05 = t(PY[is.highmoisture,])
highmoist.resids05 = data.frame(sweep(highmoist.reps05, 2, highmoist))

boxplot05 = ggplot(data=stack(highmoist.resids05), aes(x=ind, y=values)) + 
  geom_boxplot(outlier.size = 0.1) +
  geom_hline(yintercept=0, color="red") +
  stat_summary(fun="mean", geom="point", shape=23, size=1, fill="lightblue") +
  labs(x="Site", y="Residual value",
       title="Posterior predictive residuals (0.5)") +
  ylim(-0.1,0.1) +
  theme_minimal() +
  theme(axis.text.x = element_blank(), panel.grid.major = element_blank())
boxplot05

highmoist.reps10 = t(PY1[is.highmoisture,])
highmoist.resids10 = data.frame(sweep(highmoist.reps10, 2, highmoist))

boxplot10 = ggplot(data=stack(highmoist.resids10), aes(x=ind, y=values)) + 
  geom_boxplot(outlier.size = 0.1) +
  geom_hline(yintercept=0, color="red") +
  stat_summary(fun="mean", geom="point", shape=23, size=1, fill="lightblue")+
  labs(x="Site", y="Residual value",
       title="Posterior predictive residuals (1.0)") +
  ylim(-0.1,0.1) +
  theme_minimal() +
  theme(axis.text.x = element_blank(), panel.grid.major = element_blank())
boxplot10

highmoist.reps15 = t(PY1.5[is.highmoisture,])
highmoist.resids15 = data.frame(sweep(highmoist.reps15, 2, highmoist))

boxplot15 = ggplot(data=stack(highmoist.resids15), aes(x=ind, y=values)) + 
  geom_boxplot(outlier.size = 0.1) +
  geom_hline(yintercept=0, color="red") +
  stat_summary(fun="mean", geom="point", shape=23, size=1, fill="lightblue")+
  labs(x="Site", y="Residual value",
       title="Posterior predictive residuals (1.5)") +
  ylim(-0.15,0.15) +
  theme_minimal() +
  theme(axis.text.x = element_blank(), panel.grid.major = element_blank())
boxplot15

highmoist.reps25= t(PY2.5[is.highmoisture,])
highmoist.resids25 = data.frame(sweep(highmoist.reps25, 2, highmoist))

boxplot25 = ggplot(data=stack(highmoist.resids25), aes(x=ind, y=values)) + 
  geom_boxplot(outlier.size = 0.1) +
  geom_hline(yintercept=0, color="red") +
  stat_summary(fun="mean", geom="point", shape=23, size=1, fill="lightblue")+
  labs(x="Site", y="Residual value",
       title="Posterior predictive residuals (2.5)") +
  ylim(-0.15,0.15) +
  theme_minimal() +
  theme(axis.text.x = element_blank(), panel.grid.major = element_blank())
boxplot25

# 
 ggsave("figures/boxplot05-mid.pdf", boxplot05, dpi=600, width=5, height=3)
 ggsave("figures/boxplot10-mid.pdf", boxplot10, dpi=600, width=5, height=3)
# ggsave("figures/boxplot15.pdf", boxplot15, dpi=600, width=5, height=3)
# ggsave("figures/boxplot25.pdf", boxplot25, dpi=600, width=5, height=3)
```

```{r 4c quadratic loss, echo=F}
QLMeasure = function(yirep, yi, k) {
  sum(apply(yirep, 1, var)) + (k/(k+1))*sum((yi-apply(yirep, 1, mean))^2)
}

pdf("figures/QL.pdf", width=6, height=6)
par(pty="s")
curve(QLMeasure(PY, Y, x), 
      xlim=c(0,10),
      ylim=c(0.15,0.3),
      xlab="k",
      ylab="L(k)",
      col="blue",
      main="Quadratic Loss Measure")
curve(QLMeasure(PY1, Y, x), add=TRUE, col="red", lty=2)
curve(QLMeasure(PY1.5, Y, x), add=TRUE, col="green", lty=3)
curve(QLMeasure(PY2.5, Y, x), add=TRUE, col="black", lty=4)
legend("bottom", title="Smoothness", c("0.5", "1.0", "1.5", "2.5"),
       col=c("blue", "red", "green", "black"),
       lty=1:4, xpd=TRUE, seg.len=2, 
       bty="n", lwd=2,
       horiz=TRUE)
dev.off()
```

```{r}
Sigmahat0.5 = cov(t(PY))
Sigmahat1.0 = cov(t(PY1))
Sigmahat1.5 = cov(t(PY1.5))
Sigmahat2.5 = cov(t(PY2.5))

chisq0.5 = t(Y - post.mean) %*% solve(Sigmahat0.5) %*% (Y - post.mean)
chisq1.0 = t(Y - post.mean) %*% solve(Sigmahat1.0) %*% (Y - post.mean)
chisq1.5 = t(Y - post.mean) %*% solve(Sigmahat1.5) %*% (Y - post.mean)
chisq2.5 = t(Y - post.mean) %*% solve(Sigmahat2.5) %*% (Y - post.mean)

1-pchisq(chisq0.5, df = 715)
1-pchisq(chisq1.0, df = 715)
1-pchisq(chisq1.5, df = 715)
1-pchisq(chisq1.5, df = 715)
```